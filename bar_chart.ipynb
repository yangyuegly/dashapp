
# coding: utf-8

# # New Orleans Dataset
# 
# Mileva Van Tuyl
# Funing Yang
# Daphka Alius
# Jane Yang

# In[2]:

import json
import pandas as pd
#import numpy as np
from collections import OrderedDict
import plotly.graph_objs as go
import plotly
from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot


# In[ ]:

fake_news_accounts = [
'NewOrleansON',
'ElPasoTopNews',
'DailySanJose',
'ChicagoDailyNew',
'DailySanFran',
'DetroitDailyNew',
'TodayCincinnati',
'MinneapolisON',
'KansasDailyNews',
'TodayBostonMA',
'TodayPittsburgh',
'Seattle_Post',
'PhiladelphiaON',
'DailyLosAngeles',
'HoustonTopNews',
'DailySanDiego',
'DallasTopNews',
'WashingtOnline',
'TodayNYCity',
'OnlineCleveland',
'SanAntoTopNews',
'PhoenixDailyNew',
'TodayMiami',
'Atlanta_Online',
'Baltimore0nline',
'OaklandOnline',
'StLouisOnline']


# In[ ]:




# In[ ]:

def URL(allTweets):
    NOTweets = allTweets # get all New Orleans tweets
    NODTList = [tweet[2] for tweet in NOTweets] # take only the timestamp

    # create a dictionary that maps each timestamp to the count 1 (since it corresponds to one tweet)
    data = OrderedDict([('tweet_time', NODTList), ('count', [1]*len(NODTList))])

    df = pd.DataFrame.from_dict(data)
    df.head()

    # make the first column of type DateTimeIndex
    df['tweet_time'] = pd.to_datetime(df['tweet_time'])
    df = df.set_index('tweet_time')
    df.head()

    df['count'].resample('M').sum()


# In[ ]:



# In[ ]:

init_notebook_mode(connected=True)


# In[ ]:

##getting start and end dates
#startDates = []
#endDates = []
#
#with open('start-end-dates-fake-accounts.json','r') as inputFile:
#        dates = json.load(inputFile)
#    
#for acc in fake_news_accounts:
#    startDates.append((pd.to_datetime(dates[acc][0])).date())
#    endDates.append((pd.to_datetime(dates[acc][1])).date())
#
#all_datelist={}
#for i in range(len(fake_news_accounts)):
#    all_datelist[fake_news_accounts[i]]=(pd.date_range(startDates[i],endDates[i], freq='M').tolist())
#



# In[4]:

#generate x-axis dates using start and end date of NewOrleansON
startDate = pd.to_datetime("2014-09-11"); #start date for NO
endDate = pd.to_datetime("2017-08-08"); #end date for NO
datelist = (pd.date_range(startDate,endDate, freq='M').tolist())


# In[ ]:

def getTweetsFromNews(newsName):
    with open(newsName+'.txt','r') as inputFile:
        curr = json.load(inputFile)
        curr.sort(key = lambda aList: aList[13])
    return curr


# In[ ]:




# In[1]:

def URLRatio (newsName, tweets):
    counterFURLs=[]
    numFakeTweets = []    

    monthly_fakeTweets = []
    
    for i in range(len(datelist)-1):
        curr_month = []
        for ftweet in tweets: 
            curr = pd.to_datetime(ftweet[13])
            curr = curr.date()
            if curr>=datelist[i].date() and curr<=datelist[i+1].date():
                curr_month.append(ftweet)
             #   fakeNewsList.remove(ftweet)
            elif curr>datelist[i+1].date():
                #print(curr_month)
                monthly_fakeTweets.append(curr_month)
                break     
    monthly_fakeTweets.append(curr_month)  #in the case where the current month is the last
    #print(monthly_fakeTweets[3])
    
    
    numFakeTweets=[len(each_month) for each_month in monthly_fakeTweets]
    for current_month in monthly_fakeTweets:
        curr_counter = 0
        for ftweet in current_month:
            if '://' in ftweet[28]:
                #print(ftweet[28])
                curr_counter+=1                
        counterFURLs.append(curr_counter)
   #     print(counterFURLs)
   #    print(ftweet[28])
    counterNoFURLs = []
    counterNoFURLs= [i - j for i, j in zip(numFakeTweets, counterFURLs)] 
    print(newsName)
    print(counterNoFURLs)
    print(counterFURLs)


 #   ratioWithURLs=(counterFURLs/numFakeTweets)
#  ratioNoURLs=(counterNoFURLs/numFakeTweets)
    




for news in fake_news_accounts:
    no = getTweetsFromNews(news)
    URLRatio(news,no)

